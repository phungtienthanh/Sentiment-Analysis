"""
data_preparation.py
-------------------
Module d√πng ƒë·ªÉ:
1Ô∏è‚É£ L√†m s·∫°ch vƒÉn b·∫£n (HTML, emoji, stopwords, lemmatize)
2Ô∏è‚É£ T·∫£i v√† l∆∞u tokenizer c·ªßa BERT (bert-base-uncased)
3Ô∏è‚É£ Chuy·ªÉn text th√†nh input tensor (padding, truncation)
4Ô∏è‚É£ Chu·∫©n b·ªã embedding matrix t·ª´ GloVe cho LSTM
"""

import re
import string
import os
import nltk
import numpy as np
from bs4 import BeautifulSoup
import contractions
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from transformers import BertTokenizer

# --------------------------------------------------
# 1Ô∏è‚É£ C·∫•u h√¨nh v√† t·∫£i t√†i nguy√™n NLP
# --------------------------------------------------
nltk.download('stopwords')
nltk.download('wordnet')

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

# --------------------------------------------------
# 2Ô∏è‚É£ H√†m l√†m s·∫°ch vƒÉn b·∫£n
# --------------------------------------------------
def clean_text(text: str) -> str:
    """L√†m s·∫°ch vƒÉn b·∫£n: x√≥a HTML, emoji, URL, stopword, lemmatize."""
    if not isinstance(text, str):
        return ""

    # 1. X√≥a HTML
    text = BeautifulSoup(text, "html.parser").get_text()

    # 2. Chu·∫©n h√≥a contractions (don‚Äôt ‚Üí do not)
    text = contractions.fix(text)

    # 3. X√≥a emoji
    emoji_pattern = re.compile(
        "["
        u"\U0001F600-\U0001F64F"
        u"\U0001F300-\U0001F5FF"
        u"\U0001F680-\U0001F6FF"
        u"\U0001F1E0-\U0001F1FF"
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        "]+", flags=re.UNICODE)
    text = emoji_pattern.sub(r'', text)

    # 4. X√≥a URL
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)

    # 5. X√≥a d·∫•u c√¢u, chuy·ªÉn lowercase
    text = text.translate(str.maketrans('', '', string.punctuation)).lower()

    # 6. Lemmatize + lo·∫°i stopword + gi·ªØ t·ª´ alphabet
    tokens = [
        lemmatizer.lemmatize(word)
        for word in text.split()
        if word.isalpha() and word not in stop_words
    ]

    return " ".join(tokens)

# --------------------------------------------------
# 3Ô∏è‚É£ Tokenizer BERT (t·∫£i v√† l∆∞u local)
# --------------------------------------------------
def load_bert_tokenizer(save_dir: str = "./tokenizer"):
    """
    T·∫£i tokenizer c·ªßa BERT v√† l∆∞u local.
    N·∫øu ƒë√£ c√≥ local tokenizer, t·ª± ƒë·ªông load l·∫°i.
    """
    if not os.path.exists(save_dir):
        print(f"[INFO] Downloading BERT tokenizer ‚Üí {save_dir}")
        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        tokenizer.save_pretrained(save_dir)
    else:
        print(f"[INFO] Loading local BERT tokenizer from {save_dir}")
        tokenizer = BertTokenizer.from_pretrained(save_dir)
    return tokenizer

# --------------------------------------------------
# 4Ô∏è‚É£ H√†m encode vƒÉn b·∫£n
# --------------------------------------------------
def encode_texts(texts, tokenizer, max_len: int = 150, framework: str = 'pt'):
    """
    Bi·∫øn danh s√°ch vƒÉn b·∫£n th√†nh tensor input.
    framework: 'tf' ho·∫∑c 'pt' (TensorFlow ho·∫∑c PyTorch)
    """
    return tokenizer(
        texts,
        padding='max_length',
        truncation=True,
        max_length=max_len,
        return_tensors=framework
    )

# --------------------------------------------------
# 5Ô∏è‚É£ Chu·∫©n b·ªã embedding matrix (n·∫øu d√πng GloVe)
# --------------------------------------------------
def load_glove_embeddings(glove_path: str, tokenizer_vocab: dict, embedding_dim: int = 100):
    """
    T·∫°o embedding matrix t·ª´ file GloVe v√† vocab c·ªßa tokenizer.
    glove_path: ƒë∆∞·ªùng d·∫´n ƒë·∫øn glove.6B.100d.txt
    tokenizer_vocab: tokenizer.vocab (t·ª´ ‚Üí id)
    """
    print(f"[INFO] Loading GloVe embeddings from {glove_path} ...")
    embedding_index = {}
    with open(glove_path, encoding='utf-8') as f:
        for line in f:
            values = line.split()
            word = values[0]
            coefs = np.asarray(values[1:], dtype='float32')
            embedding_index[word] = coefs

    embedding_matrix = np.zeros((len(tokenizer_vocab), embedding_dim))
    for word, i in tokenizer_vocab.items():
        embedding_vector = embedding_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector

    print(f"[INFO] Embedding matrix created: {embedding_matrix.shape}")
    return embedding_matrix

# --------------------------------------------------
# 6Ô∏è‚É£ N·∫øu ch·∫°y ri√™ng file n√†y
# --------------------------------------------------
if __name__ == "__main__":
    import pandas as pd

    # V√≠ d·ª• test nh·ªè
    data = {
        "review": [
            "I loved the movie! It's absolutely fantastic <br> üòç",
            "Worst film ever... boring and too long! http://example.com",
        ],
        "sentiment": ["positive", "negative"]
    }
    df = pd.DataFrame(data)

    print("üîπ Cleaning text ...")
    df['clean_review'] = df['review'].apply(clean_text)
    print(df[['review', 'clean_review']])

    print("\nüîπ Loading tokenizer ...")
    tokenizer = load_bert_tokenizer()

    print("\nüîπ Encoding text ...")
    encoded = encode_texts(df['clean_review'].tolist(), tokenizer)
    print("Input IDs shape:", encoded['input_ids'].shape)

    # N·∫øu mu·ªën t·∫°o embedding t·ª´ GloVe (khi d√πng LSTM)
    # glove_path = './embeddings/glove.6B.100d.txt'
    # embedding_matrix = load_glove_embeddings(glove_path, tokenizer.vocab)
