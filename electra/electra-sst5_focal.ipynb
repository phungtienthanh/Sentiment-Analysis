{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T03:24:35.677848Z",
     "iopub.status.busy": "2025-12-22T03:24:35.677555Z",
     "iopub.status.idle": "2025-12-22T03:24:47.626195Z",
     "shell.execute_reply": "2025-12-22T03:24:47.625589Z",
     "shell.execute_reply.started": "2025-12-22T03:24:35.677824Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "import psutil\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, confusion_matrix,\n",
    "    precision_score, recall_score, classification_report\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import html\n",
    "import emoji\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T03:25:02.639732Z",
     "iopub.status.busy": "2025-12-22T03:25:02.638927Z",
     "iopub.status.idle": "2025-12-22T03:25:02.646242Z",
     "shell.execute_reply": "2025-12-22T03:25:02.645481Z",
     "shell.execute_reply.started": "2025-12-22T03:25:02.639705Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Configuration for ELECTRA model\n",
    "CONFIG = {\n",
    "    # Model configuration\n",
    "    'model_name': 'google/electra-base-discriminator',\n",
    "    'num_labels': 5,\n",
    "    'max_seq_length': 64,\n",
    "    \n",
    "    # Training configuration\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 2e-5,\n",
    "    'epochs': 4,\n",
    "    'warmup_steps': 500,\n",
    "    'weight_decay': 0.01,\n",
    "    'max_grad_norm': 1.0,\n",
    "    \n",
    "    # Data configuration\n",
    "    'data_dir': '/teamspace/studios/this_studio/project_web_mining/data/sst/sst-5',\n",
    "    'seed': 42,\n",
    "    'num_workers': 0,\n",
    "    \n",
    "    # Saving configuration\n",
    "    'save_dir': 'electra_models/focal/saved_models/electra_4',\n",
    "    'log_dir': 'electra_models/focal/logs/electra_4',\n",
    "    \n",
    "    # Label mapping\n",
    "    'label_list': {\n",
    "        0: \"Very Negative\",\n",
    "        1: \"Negative\", \n",
    "        2: \"Neutral\",\n",
    "        3: \"Positive\",\n",
    "        4: \"Very Positive\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(CONFIG['save_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['log_dir'], exist_ok=True)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing - SST-5 Data Loading & Analysis\n",
    "### Data preprocessing includes:\n",
    "- Loading raw SST-5 data\n",
    "- Sentence length analysis\n",
    "- Tokenization with ELECTRA tokenizer\n",
    "- Class distribution analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T03:25:05.328840Z",
     "iopub.status.busy": "2025-12-22T03:25:05.328551Z",
     "iopub.status.idle": "2025-12-22T03:25:05.369792Z",
     "shell.execute_reply": "2025-12-22T03:25:05.369190Z",
     "shell.execute_reply.started": "2025-12-22T03:25:05.328816Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_sst5_data(data_dir: str) -> Tuple[List[str], List[int], List[str], List[int], List[str], List[int]]:\n",
    "    \"\"\"\n",
    "    Load SST-5 data from CSV files\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing train.csv, dev.csv, test.csv\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (train_texts, train_labels, dev_texts, dev_labels, test_texts, test_labels)\n",
    "    \"\"\"\n",
    "    \n",
    "    train_df = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n",
    "    dev_df = pd.read_csv(os.path.join(data_dir, 'dev.csv'))\n",
    "    test_df = pd.read_csv(os.path.join(data_dir, 'test.csv'))\n",
    "    \n",
    "    train_texts = train_df['sentence'].tolist()\n",
    "    train_labels = train_df['label'].tolist()\n",
    "    \n",
    "    dev_texts = dev_df['sentence'].tolist()\n",
    "    dev_labels = dev_df['label'].tolist()\n",
    "    \n",
    "    test_texts = test_df['sentence'].tolist()\n",
    "    test_labels = test_df['label'].tolist()\n",
    "    \n",
    "    return train_texts, train_labels, dev_texts, dev_labels, test_texts, test_labels\n",
    "\n",
    "# Load data\n",
    "train_texts, train_labels, dev_texts, dev_labels, test_texts, test_labels = load_sst5_data(CONFIG['data_dir'])\n",
    "\n",
    "print(f\"Train samples: {len(train_texts)}\")\n",
    "print(f\"Dev samples: {len(dev_texts)}\")\n",
    "print(f\"Test samples: {len(test_texts)}\")\n",
    "print(f\"\\nClass distribution (Train):\")\n",
    "print(pd.Series(train_labels).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T03:25:08.898927Z",
     "iopub.status.busy": "2025-12-22T03:25:08.898398Z",
     "iopub.status.idle": "2025-12-22T03:25:08.926666Z",
     "shell.execute_reply": "2025-12-22T03:25:08.925963Z",
     "shell.execute_reply.started": "2025-12-22T03:25:08.898902Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Analyze sentence lengths\n",
    "def analyze_sentence_lengths(texts: List[str], split_name: str = \"Dataset\"):\n",
    "    \"\"\"\n",
    "    Analyze and visualize sentence length statistics\n",
    "    \"\"\"\n",
    "    lengths = [len(text.split()) for text in texts]\n",
    "    \n",
    "    stats = {\n",
    "        'min': min(lengths),\n",
    "        'max': max(lengths),\n",
    "        'mean': np.mean(lengths),\n",
    "        'std': np.std(lengths),\n",
    "        'median': np.median(lengths),\n",
    "        'p75': np.percentile(lengths, 75),\n",
    "        'p90': np.percentile(lengths, 90),\n",
    "        'p95': np.percentile(lengths, 95)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{split_name} - Sentence Length Statistics:\")\n",
    "    print(f\"  Min: {stats['min']}, Max: {stats['max']}\")\n",
    "    print(f\"  Mean: {stats['mean']:.2f}, Std: {stats['std']:.2f}\")\n",
    "    print(f\"  Median: {stats['median']:.2f}\")\n",
    "    print(f\"  75th percentile: {stats['p75']:.2f}\")\n",
    "    print(f\"  90th percentile: {stats['p90']:.2f}\")\n",
    "    print(f\"  95th percentile: {stats['p95']:.2f}\")\n",
    "    \n",
    "    return lengths, stats\n",
    "\n",
    "train_lengths, train_stats = analyze_sentence_lengths(train_texts, \"Train\")\n",
    "dev_lengths, dev_stats = analyze_sentence_lengths(dev_texts, \"Dev\")\n",
    "test_lengths, test_stats = analyze_sentence_lengths(test_texts, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T03:25:14.564341Z",
     "iopub.status.busy": "2025-12-22T03:25:14.564022Z",
     "iopub.status.idle": "2025-12-22T03:25:15.802083Z",
     "shell.execute_reply": "2025-12-22T03:25:15.801341Z",
     "shell.execute_reply.started": "2025-12-22T03:25:14.564314Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Visualize sentence length distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].hist(train_lengths, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(CONFIG['max_seq_length'], color='red', linestyle='--', label=f\"Max seq length: {CONFIG['max_seq_length']}\")\n",
    "axes[0].set_xlabel('Sentence Length (words)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Train - Sentence Length Distribution')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].hist(dev_lengths, bins=50, color='lightcoral', edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(CONFIG['max_seq_length'], color='red', linestyle='--', label=f\"Max seq length: {CONFIG['max_seq_length']}\")\n",
    "axes[1].set_xlabel('Sentence Length (words)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Dev - Sentence Length Distribution')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].hist(test_lengths, bins=50, color='lightgreen', edgecolor='black', alpha=0.7)\n",
    "axes[2].axvline(CONFIG['max_seq_length'], color='red', linestyle='--', label=f\"Max seq length: {CONFIG['max_seq_length']}\")\n",
    "axes[2].set_xlabel('Sentence Length (words)')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].set_title('Test - Sentence Length Distribution')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['log_dir'], 'sentence_length_distribution.png'), dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSentences exceeding max_seq_length ({CONFIG['max_seq_length']}):\")\n",
    "print(f\"  Train: {sum(1 for l in train_lengths if l > CONFIG['max_seq_length'])} ({100*sum(1 for l in train_lengths if l > CONFIG['max_seq_length'])/len(train_lengths):.2f}%)\")\n",
    "print(f\"  Dev: {sum(1 for l in dev_lengths if l > CONFIG['max_seq_length'])} ({100*sum(1 for l in dev_lengths if l > CONFIG['max_seq_length'])/len(dev_lengths):.2f}%)\")\n",
    "print(f\"  Test: {sum(1 for l in test_lengths if l > CONFIG['max_seq_length'])} ({100*sum(1 for l in test_lengths if l > CONFIG['max_seq_length'])/len(test_lengths):.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T03:32:54.216823Z",
     "iopub.status.busy": "2025-12-22T03:32:54.216011Z",
     "iopub.status.idle": "2025-12-22T03:32:54.833622Z",
     "shell.execute_reply": "2025-12-22T03:32:54.832871Z",
     "shell.execute_reply.started": "2025-12-22T03:32:54.216792Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "label_names = [CONFIG['label_list'][i] for i in range(CONFIG['num_labels'])]\n",
    "\n",
    "print(label_names)\n",
    "\n",
    "train_counts = pd.Series(train_labels).value_counts().sort_index()\n",
    "dev_counts = pd.Series(dev_labels).value_counts().sort_index()\n",
    "test_counts = pd.Series(test_labels).value_counts().sort_index()\n",
    "\n",
    "axes[0].bar(range(CONFIG['num_labels']), [train_counts.get(i, 0) for i in range(CONFIG['num_labels'])], color='skyblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Sentiment Class')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Train - Class Distribution')\n",
    "x = range(CONFIG['num_labels'])\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(label_names, rotation=45)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "axes[1].bar(range(CONFIG['num_labels']), [dev_counts.get(i, 0) for i in range(CONFIG['num_labels'])], color='lightcoral', edgecolor='black')\n",
    "axes[1].set_xlabel('Sentiment Class')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Dev - Class Distribution')\n",
    "x = range(CONFIG['num_labels'])\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(label_names, rotation=45)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "axes[2].bar(range(CONFIG['num_labels']), [test_counts.get(i, 0) for i in range(CONFIG['num_labels'])], color='lightgreen', edgecolor='black')\n",
    "axes[2].set_xlabel('Sentiment Class')\n",
    "axes[2].set_ylabel('Count')\n",
    "axes[2].set_title('Test - Class Distribution')\n",
    "x = range(CONFIG['num_labels'])\n",
    "axes[2].set_xticks(x)\n",
    "axes[2].set_xticklabels(label_names, rotation=45)\n",
    "\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['log_dir'], 'class_distribution.png'), dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # 1. Decode HTML entities (e.g., &amp; -> &)\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    # 2. Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # 3. Normalize URLs to [URL]\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '[URL]', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # 4. Normalize User Handles (@username) to [USER]\n",
    "    text = re.sub(r'@\\w+', '[USER]', text)\n",
    "    \n",
    "    # 5. Demojize (❤️ -> :heart:) - Giữ lại ý nghĩa cảm xúc\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "    \n",
    "    # 6. Remove excess whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # 7. (Optional) Reduce repeated characters (e.g., \"loooove\" -> \"loove\")\n",
    "    # Giúp tokenizer hoạt động tốt hơn với từ lóng\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = [preprocess_text(t) for t in train_texts]\n",
    "dev_texts = [preprocess_text(t) for t in dev_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T17:52:11.558246Z",
     "iopub.status.busy": "2025-12-19T17:52:11.557956Z",
     "iopub.status.idle": "2025-12-19T17:52:11.714581Z",
     "shell.execute_reply": "2025-12-19T17:52:11.713672Z",
     "shell.execute_reply.started": "2025-12-19T17:52:11.558222Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SST5Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for SST-5 with tokenization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, texts: List[str], labels: List[int], tokenizer, max_length: int = 64):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SST5Dataset(train_texts, train_labels, tokenizer, CONFIG['max_seq_length'])\n",
    "dev_dataset = SST5Dataset(dev_texts, dev_labels, tokenizer, CONFIG['max_seq_length'])\n",
    "test_dataset = SST5Dataset(test_texts, test_labels, tokenizer, CONFIG['max_seq_length'])\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=CONFIG['num_workers'])\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=CONFIG['num_workers'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=CONFIG['num_workers'])\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Dev batches: {len(dev_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T17:52:11.716125Z",
     "iopub.status.busy": "2025-12-19T17:52:11.715785Z",
     "iopub.status.idle": "2025-12-19T17:52:12.832403Z",
     "shell.execute_reply": "2025-12-19T17:52:12.831629Z",
     "shell.execute_reply.started": "2025-12-19T17:52:11.716093Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    CONFIG['model_name'],\n",
    "    num_labels=CONFIG['num_labels']\n",
    ")\n",
    "\n",
    "# Move to device and ensure float32 precision\n",
    "model = model.to(device)\n",
    "model = model.float()\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T17:52:12.833580Z",
     "iopub.status.busy": "2025-12-19T17:52:12.833335Z",
     "iopub.status.idle": "2025-12-19T17:52:12.840492Z",
     "shell.execute_reply": "2025-12-19T17:52:12.839549Z",
     "shell.execute_reply.started": "2025-12-19T17:52:12.833559Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Setup optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay']\n",
    ")\n",
    "\n",
    "total_steps = len(train_loader) * CONFIG['epochs']\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=CONFIG['warmup_steps'],\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"Total training steps: {total_steps}\")\n",
    "print(f\"Warmup steps: {CONFIG['warmup_steps']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=None, reduction='mean'):\n",
    "        \"\"\"\n",
    "        gamma: Tham số tập trung (focusing parameter), bài báo khuyến nghị gamma=2.0\n",
    "        alpha: Trọng số (class weights) nếu dữ liệu mất cân bằng. \n",
    "        reduction: 'mean' hoặc 'sum'\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        ce_loss = F.cross_entropy(logits, labels, reduction='none', weight=self.alpha)\n",
    "        \n",
    "        pt = torch.exp(-ce_loss)\n",
    "        \n",
    "        # Focal Loss: - (1 - pt)^gamma * log(pt)\n",
    "        # ce_loss là -log(pt)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop with Metrics Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T17:52:12.842186Z",
     "iopub.status.busy": "2025-12-19T17:52:12.841675Z",
     "iopub.status.idle": "2025-12-19T17:52:12.865080Z",
     "shell.execute_reply": "2025-12-19T17:52:12.864079Z",
     "shell.execute_reply.started": "2025-12-19T17:52:12.842161Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, scheduler, device, epoch, total_epochs, criterion):\n",
    "    \"\"\"\n",
    "    Train one epoch and return metrics\n",
    "    Using Focal loss\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{total_epochs}\")\n",
    "    \n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device).long()\n",
    "        attention_mask = batch['attention_mask'].to(device).long()\n",
    "        labels = batch['labels'].to(device).long()\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Get predictions\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['max_grad_norm'])\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        pbar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    # Calculate metrics\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'macro_f1': macro_f1,\n",
    "        'learning_rate': scheduler.get_last_lr()[0]\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate(model, eval_loader, device, criterion, split_name: str = \"Eval\"):\n",
    "    \"\"\"\n",
    "    Evaluate model and return metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(eval_loader, desc=f\"Evaluating {split_name}\"):\n",
    "            input_ids = batch['input_ids'].to(device).long()\n",
    "            attention_mask = batch['attention_mask'].to(device).long()\n",
    "            labels = batch['labels'].to(device).long()\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    avg_loss = total_loss / len(eval_loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'macro_f1': macro_f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'confusion_matrix': cm,\n",
    "        'predictions': all_preds,\n",
    "        'labels': all_labels\n",
    "    }\n",
    "\n",
    "print(\"Training and evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T17:52:12.866693Z",
     "iopub.status.busy": "2025-12-19T17:52:12.866260Z",
     "iopub.status.idle": "2025-12-19T17:53:59.408416Z",
     "shell.execute_reply": "2025-12-19T17:53:59.407543Z",
     "shell.execute_reply.started": "2025-12-19T17:52:12.866665Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_accuracy': [],\n",
    "    'train_macro_f1': [],\n",
    "    'dev_loss': [],\n",
    "    'dev_accuracy': [],\n",
    "    'dev_macro_f1': [],\n",
    "    'dev_precision': [],\n",
    "    'dev_recall': [],\n",
    "    'learning_rate': []\n",
    "}\n",
    "\n",
    "# Best model tracking\n",
    "best_dev_f1 = 0\n",
    "best_model_path = os.path.join(CONFIG['save_dir'], 'best_model.pt')\n",
    "best_dev_metrics = None\n",
    "\n",
    "# Training start time\n",
    "training_start_time = time.time()\n",
    "epoch_times = []\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "alpha = torch.tensor([1.1,1.0,1.1,1.0,1.1]).to(device)\n",
    "criterion = FocalLoss(gamma=1.0, alpha=alpha).to(device)\n",
    "\n",
    "# Main training loop\n",
    "for epoch in range(CONFIG['epochs']):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Train\n",
    "    train_metrics = train_epoch(model, train_loader, optimizer, scheduler, device, epoch, CONFIG['epochs'], criterion)\n",
    "    \n",
    "    # Evaluate on dev set\n",
    "    dev_metrics = evaluate(model, dev_loader, device, criterion, \"Dev\")\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    epoch_times.append(epoch_time)\n",
    "    \n",
    "    # Record history\n",
    "    history['train_loss'].append(train_metrics['loss'])\n",
    "    history['train_accuracy'].append(train_metrics['accuracy'])\n",
    "    history['train_macro_f1'].append(train_metrics['macro_f1'])\n",
    "    history['dev_loss'].append(dev_metrics['loss'])\n",
    "    history['dev_accuracy'].append(dev_metrics['accuracy'])\n",
    "    history['dev_macro_f1'].append(dev_metrics['macro_f1'])\n",
    "    history['dev_precision'].append(dev_metrics['precision'])\n",
    "    history['dev_recall'].append(dev_metrics['recall'])\n",
    "    history['learning_rate'].append(train_metrics['learning_rate'])\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"\\nEpoch {epoch+1}/{CONFIG['epochs']} - {epoch_time:.2f}s\")\n",
    "    print(f\"  Train - Loss: {train_metrics['loss']:.4f}, Acc: {train_metrics['accuracy']:.4f}, F1: {train_metrics['macro_f1']:.4f}\")\n",
    "    print(f\"  Dev   - Loss: {dev_metrics['loss']:.4f}, Acc: {dev_metrics['accuracy']:.4f}, F1: {dev_metrics['macro_f1']:.4f}\")\n",
    "    print(f\"           Prec: {dev_metrics['precision']:.4f}, Rec: {dev_metrics['recall']:.4f}\")\n",
    "    print(f\"  LR: {train_metrics['learning_rate']:.2e}\")\n",
    "    \n",
    "    # Save best model based on dev F1\n",
    "    if dev_metrics['macro_f1'] > best_dev_f1:\n",
    "        best_dev_f1 = dev_metrics['macro_f1']\n",
    "        best_dev_metrics = dev_metrics\n",
    "        \n",
    "        # Remove old best model if exists\n",
    "        if os.path.exists(best_model_path):\n",
    "            os.remove(best_model_path)\n",
    "        \n",
    "        # Save new best model\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"  ✓ Best model saved! (F1: {best_dev_f1:.4f})\")\n",
    "\n",
    "total_training_time = time.time() - training_start_time\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training completed in {total_training_time:.2f}s\")\n",
    "print(f\"Average time per epoch: {np.mean(epoch_times):.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T17:53:59.411166Z",
     "iopub.status.busy": "2025-12-19T17:53:59.410897Z",
     "iopub.status.idle": "2025-12-19T17:54:00.917784Z",
     "shell.execute_reply": "2025-12-19T17:54:00.916867Z",
     "shell.execute_reply.started": "2025-12-19T17:53:59.411141Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history['train_loss'], marker='o', label='Train Loss')\n",
    "axes[0, 0].plot(history['dev_loss'], marker='s', label='Dev Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Loss over Epochs')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 1].plot(history['train_accuracy'], marker='o', label='Train Accuracy')\n",
    "axes[0, 1].plot(history['dev_accuracy'], marker='s', label='Dev Accuracy')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].set_title('Accuracy over Epochs')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Macro-F1\n",
    "axes[1, 0].plot(history['train_macro_f1'], marker='o', label='Train Macro-F1')\n",
    "axes[1, 0].plot(history['dev_macro_f1'], marker='s', label='Dev Macro-F1')\n",
    "axes[1, 0].plot(history['dev_precision'], marker='^', label='Dev Precision')\n",
    "axes[1, 0].plot(history['dev_recall'], marker='d', label='Dev Recall')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Score')\n",
    "axes[1, 0].set_title('Macro-F1, Precision, Recall over Epochs')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "axes[1, 1].plot(history['learning_rate'], marker='o', color='green')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Learning Rate')\n",
    "axes[1, 1].set_title('Learning Rate Schedule')\n",
    "axes[1, 1].set_yscale('log')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['log_dir'], 'training_history.png'), dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Set Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T17:54:00.919589Z",
     "iopub.status.busy": "2025-12-19T17:54:00.919012Z",
     "iopub.status.idle": "2025-12-19T17:54:10.225452Z",
     "shell.execute_reply": "2025-12-19T17:54:10.224443Z",
     "shell.execute_reply.started": "2025-12-19T17:54:00.919563Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load best model for test evaluation\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# Evaluate on test set\n",
    "test_metrics = evaluate(model, test_loader, device, criterion, \"Test\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Test Set Performance:\")\n",
    "print(f\"  Accuracy:     {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"  Macro-F1:     {test_metrics['macro_f1']:.4f}\")\n",
    "print(f\"  Precision:    {test_metrics['precision']:.4f}\")\n",
    "print(f\"  Recall:       {test_metrics['recall']:.4f}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T17:54:10.226988Z",
     "iopub.status.busy": "2025-12-19T17:54:10.226698Z",
     "iopub.status.idle": "2025-12-19T17:54:11.156233Z",
     "shell.execute_reply": "2025-12-19T17:54:11.155452Z",
     "shell.execute_reply.started": "2025-12-19T17:54:10.226964Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "label_names = [CONFIG['label_list'][i] for i in range(CONFIG['num_labels'])]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Dev confusion matrix\n",
    "sns.heatmap(best_dev_metrics['confusion_matrix'], annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_names, yticklabels=label_names, ax=axes[0], cbar_kws={'label': 'Count'})\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('True')\n",
    "axes[0].set_title('Dev Set - Confusion Matrix')\n",
    "\n",
    "# Test confusion matrix\n",
    "sns.heatmap(test_metrics['confusion_matrix'], annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=label_names, yticklabels=label_names, ax=axes[1], cbar_kws={'label': 'Count'})\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('True')\n",
    "axes[1].set_title('Test Set - Confusion Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['log_dir'], 'confusion_matrices.png'), dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T17:54:11.157557Z",
     "iopub.status.busy": "2025-12-19T17:54:11.157227Z",
     "iopub.status.idle": "2025-12-19T17:54:11.176216Z",
     "shell.execute_reply": "2025-12-19T17:54:11.175326Z",
     "shell.execute_reply.started": "2025-12-19T17:54:11.157533Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(\"\\nTest Set - Classification Report:\")\n",
    "print(\"=\"*60)\n",
    "report = classification_report(\n",
    "    test_metrics['labels'],\n",
    "    test_metrics['predictions'],\n",
    "    target_names=label_names,\n",
    "    digits=4\n",
    ")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Error Analysis - Misclassification Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T17:54:11.177600Z",
     "iopub.status.busy": "2025-12-19T17:54:11.177304Z",
     "iopub.status.idle": "2025-12-19T17:54:11.191573Z",
     "shell.execute_reply": "2025-12-19T17:54:11.190687Z",
     "shell.execute_reply.started": "2025-12-19T17:54:11.177576Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def analyze_prediction_errors(y_true, y_pred, label_names, split_name=\"Test\"):\n",
    "    \"\"\"\n",
    "    Analyze misclassification patterns\n",
    "    \"\"\"\n",
    "    error_categories = {\n",
    "        'off_by_1': [],\n",
    "        'off_by_2': [],\n",
    "        'off_by_3_or_more': []\n",
    "    }\n",
    "    \n",
    "    for true_label, pred_label in zip(y_true, y_pred):\n",
    "        if true_label != pred_label:\n",
    "            diff = abs(true_label - pred_label)\n",
    "            if diff == 1:\n",
    "                error_categories['off_by_1'].append((true_label, pred_label))\n",
    "            elif diff == 2:\n",
    "                error_categories['off_by_2'].append((true_label, pred_label))\n",
    "            else:\n",
    "                error_categories['off_by_3_or_more'].append((true_label, pred_label))\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{split_name} Set - Error Analysis:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total errors: {len(y_true) - sum(1 for t, p in zip(y_true, y_pred) if t == p)}\")\n",
    "    print(f\"\\nMisclassification patterns:\")\n",
    "    print(f\"  Off by 1 label:         {len(error_categories['off_by_1'])} errors\")\n",
    "    print(f\"  Off by 2 labels:        {len(error_categories['off_by_2'])} errors\")\n",
    "    print(f\"  Off by 3+ labels:       {len(error_categories['off_by_3_or_more'])} errors\")\n",
    "    \n",
    "    # Analyze off-by-1 errors\n",
    "    if error_categories['off_by_1']:\n",
    "        print(f\"\\nOff-by-1 error patterns:\")\n",
    "        off_by_1_patterns = defaultdict(int)\n",
    "        for true_label, pred_label in error_categories['off_by_1']:\n",
    "            pattern = f\"{label_names[true_label]} → {label_names[pred_label]}\"\n",
    "            off_by_1_patterns[pattern] += 1\n",
    "        \n",
    "        for pattern, count in sorted(off_by_1_patterns.items(), key=lambda x: -x[1]):\n",
    "            print(f\"  {pattern}: {count}\")\n",
    "    \n",
    "    # Analyze off-by-2 errors\n",
    "    if error_categories['off_by_2']:\n",
    "        print(f\"\\nOff-by-2 error patterns:\")\n",
    "        off_by_2_patterns = defaultdict(int)\n",
    "        for true_label, pred_label in error_categories['off_by_2']:\n",
    "            pattern = f\"{label_names[true_label]} → {label_names[pred_label]}\"\n",
    "            off_by_2_patterns[pattern] += 1\n",
    "        \n",
    "        for pattern, count in sorted(off_by_2_patterns.items(), key=lambda x: -x[1]):\n",
    "            print(f\"  {pattern}: {count}\")\n",
    "    \n",
    "    # Analyze off-by-3+ errors\n",
    "    if error_categories['off_by_3_or_more']:\n",
    "        print(f\"\\nOff-by-3+ error patterns:\")\n",
    "        off_by_3_patterns = defaultdict(int)\n",
    "        for true_label, pred_label in error_categories['off_by_3_or_more']:\n",
    "            pattern = f\"{label_names[true_label]} → {label_names[pred_label]}\"\n",
    "            off_by_3_patterns[pattern] += 1\n",
    "        \n",
    "        for pattern, count in sorted(off_by_3_patterns.items(), key=lambda x: -x[1]):\n",
    "            print(f\"  {pattern}: {count}\")\n",
    "    \n",
    "    return error_categories\n",
    "\n",
    "error_analysis = analyze_prediction_errors(\n",
    "    test_metrics['labels'],\n",
    "    test_metrics['predictions'],\n",
    "    label_names,\n",
    "    \"Test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Inference Test - Analyzing Specific Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T17:54:11.193531Z",
     "iopub.status.busy": "2025-12-19T17:54:11.192831Z",
     "iopub.status.idle": "2025-12-19T17:54:11.301184Z",
     "shell.execute_reply": "2025-12-19T17:54:11.300241Z",
     "shell.execute_reply.started": "2025-12-19T17:54:11.193504Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(text: str, model, tokenizer, device, label_names):\n",
    "    \"\"\"\n",
    "    Predict sentiment for a single text\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=CONFIG['max_seq_length'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "    \n",
    "    pred_label = torch.argmax(logits, dim=1).item()\n",
    "    confidence = probs[0, pred_label].item()\n",
    "    \n",
    "    return pred_label, confidence, probs[0].cpu().numpy()\n",
    "\n",
    "# Test sentences - mix of positive and negative words but opposite meanings\n",
    "test_sentences = [\n",
    "    \"This movie is not good at all, I didn't like it.\",  # Negative despite \"good\"\n",
    "    \"I love this terrible movie!\",  # Positive despite \"terrible\"\n",
    "    \"This is amazing!\",  # Positive\n",
    "    \"This is awful and terrible.\",  # Negative\n",
    "    \"The movie was okay, nothing special.\",  # Neutral\n",
    "    \"I absolutely hate this masterpiece!\",  # Negative despite \"masterpiece\"\n",
    "    \"This is the worst best movie ever made.\",  # Contradictory\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Inference Test - Sentiment Predictions for Specific Sentences\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for text in test_sentences:\n",
    "    pred_label, confidence, probs = predict_sentiment(text, model, tokenizer, device, label_names)\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"  Prediction: {label_names[pred_label]} (confidence: {confidence:.4f})\")\n",
    "    print(f\"  Probabilities:\")\n",
    "    for i, prob in enumerate(probs):\n",
    "        print(f\"    {label_names[i]:15s}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Training Logs and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T17:54:11.303443Z",
     "iopub.status.busy": "2025-12-19T17:54:11.302657Z",
     "iopub.status.idle": "2025-12-19T17:54:11.313618Z",
     "shell.execute_reply": "2025-12-19T17:54:11.312612Z",
     "shell.execute_reply.started": "2025-12-19T17:54:11.303414Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prepare comprehensive training log\n",
    "training_log = {\n",
    "    'model': CONFIG['model_name'],\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'total_training_time': total_training_time,\n",
    "    'average_epoch_time': np.mean(epoch_times),\n",
    "    'config': CONFIG,\n",
    "    'training_history': history,\n",
    "    'test_metrics': {\n",
    "        'accuracy': float(test_metrics['accuracy']),\n",
    "        'macro_f1': float(test_metrics['macro_f1']),\n",
    "        'precision': float(test_metrics['precision']),\n",
    "        'recall': float(test_metrics['recall'])\n",
    "    },\n",
    "    'best_dev_metrics': {\n",
    "        'accuracy': float(best_dev_metrics['accuracy']),\n",
    "        'macro_f1': float(best_dev_metrics['macro_f1']),\n",
    "        'precision': float(best_dev_metrics['precision']),\n",
    "        'recall': float(best_dev_metrics['recall'])\n",
    "    },\n",
    "    'gpu_memory': {\n",
    "        'total_gb': torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0,\n",
    "        'allocated_gb': torch.cuda.memory_allocated(0) / 1e9 if torch.cuda.is_available() else 0\n",
    "    },\n",
    "    'model_parameters': {\n",
    "        'total': int(total_params),\n",
    "        'trainable': int(trainable_params)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save log as JSON\n",
    "log_path = os.path.join(CONFIG['log_dir'], 'training_log.json')\n",
    "with open(log_path, 'w') as f:\n",
    "    json.dump(training_log, f, indent=2)\n",
    "\n",
    "print(f\"Training log saved to {log_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T17:54:11.314984Z",
     "iopub.status.busy": "2025-12-19T17:54:11.314641Z",
     "iopub.status.idle": "2025-12-19T17:54:11.335907Z",
     "shell.execute_reply": "2025-12-19T17:54:11.335155Z",
     "shell.execute_reply.started": "2025-12-19T17:54:11.314946Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "summary_report = f\"\"\"\n",
    "{'='*80}\n",
    "SST-5 SENTIMENT ANALYSIS - ELECTRA MODEL TRAINING SUMMARY\n",
    "{'='*80}\n",
    "\n",
    "MODEL CONFIGURATION:\n",
    "  Model Name:           {CONFIG['model_name']}\n",
    "  Total Parameters:     {total_params:,}\n",
    "  Trainable Parameters: {trainable_params:,}\n",
    "\n",
    "TRAINING CONFIGURATION:\n",
    "  Batch Size:           {CONFIG['batch_size']}\n",
    "  Learning Rate:        {CONFIG['learning_rate']}\n",
    "  Epochs:               {CONFIG['epochs']}\n",
    "  Warmup Steps:         {CONFIG['warmup_steps']}\n",
    "  Max Sequence Length:  {CONFIG['max_seq_length']}\n",
    "  Weight Decay:         {CONFIG['weight_decay']}\n",
    "\n",
    "TRAINING RESULTS:\n",
    "  Total Training Time:  {total_training_time:.2f} seconds ({total_training_time/60:.2f} minutes)\n",
    "  Average Epoch Time:   {np.mean(epoch_times):.2f} seconds\n",
    "  GPU Memory:           {training_log['gpu_memory']['total_gb']:.2f} GB total, {training_log['gpu_memory']['allocated_gb']:.2f} GB used\n",
    "\n",
    "DEVELOPMENT SET PERFORMANCE (Best Model):\n",
    "  Accuracy:             {best_dev_metrics['accuracy']:.4f}\n",
    "  Macro-F1:             {best_dev_metrics['macro_f1']:.4f}\n",
    "  Precision:            {best_dev_metrics['precision']:.4f}\n",
    "  Recall:               {best_dev_metrics['recall']:.4f}\n",
    "\n",
    "TEST SET PERFORMANCE:\n",
    "  Accuracy:             {test_metrics['accuracy']:.4f}\n",
    "  Macro-F1:             {test_metrics['macro_f1']:.4f}\n",
    "  Precision:            {test_metrics['precision']:.4f}\n",
    "  Recall:               {test_metrics['recall']:.4f}\n",
    "\n",
    "DATA STATISTICS:\n",
    "  Train samples:        {len(train_texts)}\n",
    "  Dev samples:          {len(dev_texts)}\n",
    "  Test samples:         {len(test_texts)}\n",
    "  Average sentence length: {np.mean(train_lengths):.2f} words\n",
    "  Max sentence length:  {np.max(train_lengths)} words\n",
    "\n",
    "SAVED ARTIFACTS:\n",
    "  Best Model:           {best_model_path}\n",
    "  Training Log:         {log_path}\n",
    "  Visualizations:       {CONFIG['log_dir']}/\n",
    "\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "print(summary_report)\n",
    "\n",
    "# Save summary report\n",
    "report_path = os.path.join(CONFIG['log_dir'], 'summary_report.txt')\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(f\"\\nSummary report saved to {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_length_analysis_stacked(model, test_loader, test_texts, test_labels, device, config):\n",
    "    \"\"\"\n",
    "    Analyze and visualize prediction accuracy/errors based on sentence length\n",
    "    Displays a stacked bar chart with correct predictions at bottom and errors on top\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    \n",
    "    print(\"Đang thu thập dự đoán trên tập Test...\")\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(test_labels)\n",
    "    all_lengths = np.array([len(t.split()) for t in test_texts])\n",
    "    \n",
    "    # 1. Tạo các bins (chia khoảng độ dài câu)\n",
    "    max_len = max(all_lengths)\n",
    "    bins = np.linspace(0, max_len, 16)  # Chia làm 15 khoảng\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "    width = (bins[1] - bins[0]) * 0.8  # Độ rộng cột\n",
    "\n",
    "    correct_counts = []\n",
    "    incorrect_counts = []\n",
    "    accuracy_rates = []\n",
    "    \n",
    "    for i in range(len(bins)-1):\n",
    "        mask = (all_lengths >= bins[i]) & (all_lengths < bins[i+1])\n",
    "        if np.any(mask):\n",
    "            bin_preds = all_preds[mask]\n",
    "            bin_labels = all_labels[mask]\n",
    "            \n",
    "            correct = np.sum(bin_preds == bin_labels)\n",
    "            incorrect = np.sum(bin_preds != bin_labels)\n",
    "            total = correct + incorrect\n",
    "            accuracy = (correct / total * 100) if total > 0 else 0\n",
    "            \n",
    "            correct_counts.append(correct)\n",
    "            incorrect_counts.append(incorrect)\n",
    "            accuracy_rates.append(accuracy)\n",
    "        else:\n",
    "            correct_counts.append(0)\n",
    "            incorrect_counts.append(0)\n",
    "            accuracy_rates.append(0)\n",
    "\n",
    "    # 2. VẼ BIỂU ĐỒ CỘT CHỒNG\n",
    "    fig, ax = plt.subplots(figsize=(14, 8), facecolor='white')\n",
    "    \n",
    "    # Màu sắc chuyên nghiệp (Xanh cho Đúng, Đỏ cho Sai)\n",
    "    color_correct = '#27ae60'    # Emerald Green\n",
    "    color_incorrect = '#e74c3c'  # Alizarin Red\n",
    "    \n",
    "    # Vẽ cột Đúng ở dưới\n",
    "    ax.bar(bin_centers, correct_counts, width=width, \n",
    "            label='Correct Predictions', color=color_correct, \n",
    "            alpha=0.85, edgecolor='white', linewidth=0.5)\n",
    "    \n",
    "    # Vẽ cột Sai chồng lên trên (bottom=correct_counts)\n",
    "    ax.bar(bin_centers, incorrect_counts, width=width, \n",
    "            bottom=correct_counts, label='Misclassified', \n",
    "            color=color_incorrect, alpha=0.9, \n",
    "            edgecolor='white', linewidth=0.5)\n",
    "\n",
    "    # 3. Tinh chỉnh hiển thị\n",
    "    ax.set_title('Stacked Analysis: Accuracy vs. Error by Sentence Length (ELECTRA)', \n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.set_xlabel('Sentence Length (Number of words)', fontsize=13)\n",
    "    ax.set_ylabel('Number of Sentences', fontsize=13)\n",
    "    \n",
    "    # Thêm đường kẻ ngang cho dễ nhìn\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Legend\n",
    "    ax.legend(frameon=True, shadow=True, fontsize=11, loc='upper right')\n",
    "\n",
    "    # Hiển thị tỷ lệ % câu đúng trên đầu mỗi cột\n",
    "    for i in range(len(bin_centers)):\n",
    "        total = correct_counts[i] + incorrect_counts[i]\n",
    "        if total >= 5:  # Chỉ hiện nếu cột đủ cao để nhìn\n",
    "            acc_rate = accuracy_rates[i]\n",
    "            ax.text(bin_centers[i], total + 1, f'{100 - acc_rate:.0f}%', \n",
    "                     ha='center', va='bottom', fontsize=10, \n",
    "                     color='#e74c3c', fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(config['log_dir'], 'stacked_error_analysis.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"✓ Biểu đồ cột chồng đã được lưu tại: {save_path}\")\n",
    "\n",
    "print(\"Hàm plot_length_analysis_stacked đã được định nghĩa.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chạy phân tích độ dài câu\n",
    "print(\"Đang thực hiện phân tích độ dài câu trên tập Test...\\n\")\n",
    "plot_length_analysis_stacked(model, test_loader, test_texts, test_labels, device, CONFIG)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 9076430,
     "sourceId": 14227479,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
