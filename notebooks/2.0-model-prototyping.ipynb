{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "370a34ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell5530\\Sentiment-Analysis\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tqdm import tqdm # Th∆∞ vi·ªán h·ªØu √≠ch ƒë·ªÉ xem thanh ti·∫øn tr√¨nh\n",
    "import warnings     # ƒê·ªÉ t·∫Øt c√°c c·∫£nh b√°o kh√¥ng c·∫ßn thi·∫øt\n",
    "import os\n",
    "\n",
    "# C√†i ƒë·∫∑t\n",
    "warnings.filterwarnings('ignore') # B·ªè qua c√°c c·∫£nh b√°o\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # T·∫Øt c·∫£nh b√°o c·ªßa tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f32a43bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# config.py\n",
    "BERT_MODEL_NAME = 'bert-base-uncased'\n",
    "DATASET_NAME = 'SetFit/sst5'\n",
    "\n",
    "# Quy·∫øt ƒë·ªãnh t·ª´ file 1.0\n",
    "MAX_LENGTH = 64 \n",
    "\n",
    "# C·∫•u h√¨nh th·ª≠ nghi·ªám\n",
    "BATCH_SIZE = 8 # Ch·ªâ c·∫ßn 8 m·∫´u ƒë·ªÉ th·ª≠\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "# C·∫•u h√¨nh m√¥ h√¨nh\n",
    "LSTM_HIDDEN_SIZE = 256 # K√≠ch th∆∞·ªõc l·ªõp ·∫©n LSTM\n",
    "LSTM_LAYERS = 2      # S·ªë l·ªõp BiLSTM (stacked)\n",
    "DROPOUT_RATE = 0.3\n",
    "NUM_CLASSES = 5      # 5 l·ªõp sentiment\n",
    "\n",
    "# Thi·∫øt b·ªã (r·∫•t quan tr·ªçng)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7536b289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Test Tokenizer ---\n",
      "Input IDs shape: torch.Size([1, 64])\n",
      "Attention Mask shape: torch.Size([1, 64])\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "\n",
    "# Test th·ª≠ tokenizer\n",
    "test_text = \"This movie is not bad, it's actually great!\"\n",
    "encoding = tokenizer.encode_plus(\n",
    "    test_text,\n",
    "    add_special_tokens=True,    # Th√™m [CLS] v√† [SEP]\n",
    "    max_length=MAX_LENGTH,      # Pad ho·∫∑c Truncate\n",
    "    padding='max_length',       # Pad t·ªõi max_length\n",
    "    truncation=True,            # C·∫Øt n·∫øu d√†i h∆°n max_length\n",
    "    return_tensors='pt'         # Tr·∫£ v·ªÅ PyTorch tensor\n",
    ")\n",
    "\n",
    "print(\"--- Test Tokenizer ---\")\n",
    "print(\"Input IDs shape:\", encoding['input_ids'].shape)\n",
    "print(\"Attention Mask shape:\", encoding['attention_mask'].shape)\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caae26e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SST5PrototypeDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # L·∫•y m·∫´u d·ªØ li·ªáu\n",
    "        sample = self.data[idx]\n",
    "        text = sample['text']\n",
    "        label = sample['label']\n",
    "        \n",
    "        # Tokenize vƒÉn b·∫£n\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # .squeeze() ƒë·ªÉ lo·∫°i b·ªè chi·ªÅu batch (v√¨ tokenizer tr·∫£ v·ªÅ [1, max_length])\n",
    "        # .to(torch.long) l√† c·∫ßn thi·∫øt cho label\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65850760",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLSTMClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertLSTMClassifier, self).__init__()\n",
    "        \n",
    "        # --- 1. L·ªõp BERT (Feature Extractor) ---\n",
    "        self.bert = BertModel.from_pretrained(BERT_MODEL_NAME)\n",
    "        # **ƒê√ìNG BƒÇNG BERT**\n",
    "        # kh√¥ng train l·∫°i BERT, ch·ªâ d√πng n√≥ ƒë·ªÉ tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        bert_output_size = self.bert.config.hidden_size # (ƒê√¢y l√† 768)\n",
    "        \n",
    "        # --- 2. L·ªõp Stacked BiLSTM (Encoder) ---\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=bert_output_size,         # ƒê·∫ßu v√†o l√† 768\n",
    "            hidden_size=LSTM_HIDDEN_SIZE,  # 256\n",
    "            num_layers=LSTM_LAYERS,        # 2\n",
    "            bidirectional=True,                # BiLSTM\n",
    "            batch_first=True,                  # R·∫•t quan tr·ªçng! [Batch, Seq, Feature]\n",
    "            dropout=DROPOUT_RATE if LSTM_LAYERS > 1 else 0\n",
    "        )\n",
    "        \n",
    "        lstm_output_size = LSTM_HIDDEN_SIZE * 2 # (256 * 2 = 512, v√¨ l√† BiLSTM)\n",
    "        \n",
    "        # --- 3. L·ªõp Attention ---\n",
    "        # Ch√∫ng ta s·∫Ω h·ªçc m·ªôt \"tr·ªçng s·ªë\" cho m·ªói hidden state c·ªßa LSTM\n",
    "        # (c∆° ch·∫ø Attention ƒë∆°n gi·∫£n v√† ph·ªï bi·∫øn)\n",
    "        # W*h + b\n",
    "        self.attention_weights = nn.Linear(lstm_output_size, 1)\n",
    "        \n",
    "        # --- 4. L·ªõp Ph√¢n lo·∫°i (Classifier) ---\n",
    "        self.dropout = nn.Dropout(DROPOUT_RATE)\n",
    "        self.classifier = nn.Linear(\n",
    "            lstm_output_size, # 512\n",
    "            NUM_CLASSES       # 5\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # 1. BERT: [B, S] -> [B, S, 768] (B=Batch Size, S=Seq Length)\n",
    "        # last_hidden_state ch√≠nh l√† features\n",
    "        bert_output = self.bert(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        bert_features = bert_output.last_hidden_state\n",
    "        \n",
    "        # 2. BiLSTM: [B, S, 768] -> [B, S, 512]\n",
    "        # lstm_output ch·ª©a hidden state c·ªßa *t·∫•t c·∫£* c√°c time step\n",
    "        lstm_output, (h_n, c_n) = self.lstm(bert_features)\n",
    "        \n",
    "        # 3. Attention: [B, S, 512] -> [B, 512]\n",
    "        # T√≠nh \"ƒëi·ªÉm s·ªë\" (logits) cho m·ªói t·ª´\n",
    "        # (B, S, 512) -> (B, S, 1)\n",
    "        attn_logits = self.attention_weights(lstm_output)\n",
    "        \n",
    "        # Ch√∫ng ta kh√¥ng mu·ªën Attention ch√∫ √Ω v√†o c√°c token [PAD]\n",
    "        # T·∫°o m·ªôt mask ng∆∞·ª£c l·∫°i t·ª´ attention_mask c·ªßa BERT\n",
    "        # (B, S) -> (B, S, 1)\n",
    "        attn_mask = attention_mask.unsqueeze(2)\n",
    "        # Fill c√°c v·ªã tr√≠ [PAD] (mask=0) b·∫±ng -infinity ƒë·ªÉ softmax = 0\n",
    "        attn_logits = attn_logits.masked_fill(attn_mask == 0, -1e9)\n",
    "        \n",
    "        # √Åp d·ª•ng softmax ƒë·ªÉ ra tr·ªçng s·ªë\n",
    "        # (B, S, 1)\n",
    "        attn_scores = F.softmax(attn_logits, dim=1) \n",
    "        \n",
    "        # T√≠nh \"vector ng·ªØ c·∫£nh\" (weighted sum)\n",
    "        # (B, S, 512) * (B, S, 1) -> (B, S, 512)\n",
    "        # .sum(dim=1) -> (B, 512)\n",
    "        context_vector = torch.sum(lstm_output * attn_scores, dim=1)\n",
    "        \n",
    "        # 4. Classifier: [B, 512] -> [B, 5]\n",
    "        context_vector = self.dropout(context_vector)\n",
    "        logits = self.classifier(context_vector)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a31175f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- B·∫ÆT ƒê·∫¶U KI·ªÇM TH·ª¨ PIPELINE ---\n",
      "[1/8] ƒêang t·∫£i 16 m·∫´u t·ª´ 'SetFit/sst5'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     T·∫£i xong!\n",
      "[2/8] ƒêang t·∫°o Prototype Dataset...\n",
      "[3/8] ƒêang t·∫°o DataLoader...\n",
      "[4/8] ƒêang l·∫•y 1 batch d·ªØ li·ªáu...\n",
      "     Input IDs shape: torch.Size([8, 64])\n",
      "     Attention Mask shape: torch.Size([8, 64])\n",
      "     Labels shape: torch.Size([8])\n",
      "[5/8] ƒêang kh·ªüi t·∫°o m√¥ h√¨nh BertLSTMClassifier...\n",
      "[6/8] ƒêang kh·ªüi t·∫°o Optimizer v√† Loss Function...\n",
      "[7/8] ƒêang th·ª±c hi·ªán Forward Pass (model(inputs))...\n",
      "     Output Logits shape: torch.Size([8, 5])\n",
      "     Forward Pass TH√ÄNH C√îNG!\n",
      "[8/8] ƒêang th·ª±c hi·ªán Backward Pass (loss.backward())...\n",
      "     T√≠nh ƒë∆∞·ª£c Loss: 1.6324\n",
      "     Backward Pass TH√ÄNH C√îNG!\n",
      "\n",
      "--- KI·ªÇM TH·ª¨ HO√ÄN T·∫§T --- üéâ\n",
      "To√†n b·ªô pipeline ho·∫°t ƒë·ªông t·ªët. K√≠ch th∆∞·ªõc tensor ch√≠nh x√°c.\n",
      "S·∫µn s√†ng ƒë·ªÉ chuy·ªÉn code n√†y sang c√°c file .py trong 'src/'!\n"
     ]
    }
   ],
   "source": [
    "print(\"--- B·∫ÆT ƒê·∫¶U KI·ªÇM TH·ª¨ PIPELINE ---\")\n",
    "\n",
    "try:\n",
    "    # 1. T·∫£i m·ªôt ph·∫ßn nh·ªè c·ªßa d·ªØ li·ªáu\n",
    "    print(f\"[1/8] ƒêang t·∫£i {BATCH_SIZE * 2} m·∫´u t·ª´ '{DATASET_NAME}'...\")\n",
    "    raw_datasets = load_dataset(DATASET_NAME)\n",
    "    small_train_data = raw_datasets['train'].select(range(BATCH_SIZE * 2))\n",
    "    print(\"     T·∫£i xong!\")\n",
    "\n",
    "    # 2. T·∫°o Dataset\n",
    "    print(\"[2/8] ƒêang t·∫°o Prototype Dataset...\")\n",
    "    train_dataset = SST5PrototypeDataset(\n",
    "        data=small_train_data,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "    # 3. T·∫°o DataLoader\n",
    "    print(\"[3/8] ƒêang t·∫°o DataLoader...\")\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # 4. L·∫•y M·ªòT BATCH d·ªØ li·ªáu\n",
    "    print(\"[4/8] ƒêang l·∫•y 1 batch d·ªØ li·ªáu...\")\n",
    "    batch = next(iter(train_loader))\n",
    "    \n",
    "    # Chuy·ªÉn batch l√™n DEVICE\n",
    "    input_ids = batch['input_ids'].to(DEVICE)\n",
    "    attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "    labels = batch['label'].to(DEVICE)\n",
    "    \n",
    "    print(f\"     Input IDs shape: {input_ids.shape}\")\n",
    "    print(f\"     Attention Mask shape: {attention_mask.shape}\")\n",
    "    print(f\"     Labels shape: {labels.shape}\")\n",
    "    \n",
    "    assert input_ids.shape == (BATCH_SIZE, MAX_LENGTH)\n",
    "\n",
    "    # 5. Kh·ªüi t·∫°o M√¥ h√¨nh (d√πng class ·ªü Cell 5)\n",
    "    print(\"[5/8] ƒêang kh·ªüi t·∫°o m√¥ h√¨nh BertLSTMClassifier...\")\n",
    "    model = BertLSTMClassifier().to(DEVICE)\n",
    "    # print(model) # B·ªè comment n·∫øu xem ki·∫øn tr√∫c\n",
    "\n",
    "    # 6. Kh·ªüi t·∫°o Optimizer v√† Loss (d√πng nn v√† torch.optim t·ª´ Cell 1)\n",
    "    print(\"[6/8] ƒêang kh·ªüi t·∫°o Optimizer v√† Loss Function...\")\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 7. CH·∫†Y FORWARD PASS\n",
    "    print(\"[7/8] ƒêang th·ª±c hi·ªán Forward Pass (model(inputs))...\")\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(input_ids, attention_mask)\n",
    "    \n",
    "    print(f\"     Output Logits shape: {logits.shape}\")\n",
    "    assert logits.shape == (BATCH_SIZE, NUM_CLASSES)\n",
    "    print(\"     Forward Pass TH√ÄNH C√îNG!\")\n",
    "\n",
    "    # 8. T√çNH LOSS V√Ä BACKWARD PASS\n",
    "    print(\"[8/8] ƒêang th·ª±c hi·ªán Backward Pass (loss.backward())...\")\n",
    "    loss = criterion(logits, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"     T√≠nh ƒë∆∞·ª£c Loss: {loss.item():.4f}\")\n",
    "    print(\"     Backward Pass TH√ÄNH C√îNG!\")\n",
    "    \n",
    "    print(\"\\n--- KI·ªÇM TH·ª¨ HO√ÄN T·∫§T --- üéâ\")\n",
    "    print(\"To√†n b·ªô pipeline ho·∫°t ƒë·ªông t·ªët. K√≠ch th∆∞·ªõc tensor ch√≠nh x√°c.\")\n",
    "    print(\"S·∫µn s√†ng ƒë·ªÉ chuy·ªÉn code n√†y sang c√°c file .py trong 'src/'!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- !!! G·∫∂P L·ªñI !!! ---\")\n",
    "    print(f\"L·ªói: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
