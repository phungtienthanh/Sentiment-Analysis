import joblib
import os
import re
import string
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
from nltk.tokenize import word_tokenize
from datasets import load_dataset
from sklearn.metrics import accuracy_score, f1_score, classification_report

# --- 1. KHAI B√ÅO H√ÄM X·ª¨ L√ù (B·∫ÆT BU·ªòC ƒê·ªÇ LOAD PIPELINE) ---
# C√°c h√†m n√†y ph·∫£i gi·ªëng h·ªát trong file train_svm.py, train_xgb.py...
def download_nltk_resources():
    resources = ['punkt', 'punkt_tab']
    for res in resources:
        try:
            if res == 'punkt': nltk.data.find('tokenizers/punkt')
            else: nltk.data.find('tokenizers/punkt_tab')
        except LookupError:
            nltk.download(res, quiet=True)

download_nltk_resources()

def custom_preprocessor(text):
    if not isinstance(text, str): return ""
    text = text.lower().strip()
    text = re.sub(r'\s+', ' ', text)
    return text

def custom_tokenizer(text):
    tokens = word_tokenize(text)
    filter_punct = string.punctuation.replace('!', '').replace('?', '')
    return [t for t in tokens if t not in filter_punct]

# --- 2. C·∫§U H√åNH T√äN FILE ---
# Dictionary map: "T√™n Hi·ªÉn Th·ªã" -> "T√™n File Model"
model_files = {
    "Logistic Regression": "sst5_logistic_regression_pro.joblib",
    "SVM (Linear)":        "sst5_svm_pro.joblib",
    "Random Forest":       "sst5_random_forest.joblib",
    "XGBoost":             "sst5_xgboost.joblib",
    "k-NN":                "sst5_knn.joblib",
    "Naive Bayes (CountVec)": "sst5_countvec_model.joblib" # File n√†y c·∫ßn x·ª≠ l√Ω ƒë·∫∑c bi·ªát
}

OUTPUT_FOLDER = "comparison/test"
if not os.path.exists(OUTPUT_FOLDER):
    os.makedirs(OUTPUT_FOLDER)

# --- 3. MAIN FUNCTION ---
def main():

    dataset = load_dataset("SetFit/sst5")
    X_test = dataset['test']['text']
    y_test = dataset['test']['label']
    
    label_map = {0: 'Very Negative', 1: 'Negative', 2: 'Neutral', 3: 'Positive', 4: 'Very Positive'}
    results = []
    
    
    for model_name, filename in model_files.items():
        print(f"üîπ ƒêang x·ª≠ l√Ω: {model_name}...")
        
        if not os.path.exists(filename):
            print(f"   ‚ö†Ô∏è L·ªói: Kh√¥ng t√¨m th·∫•y file '{filename}'. B·ªè qua.")
            continue

        try:
            y_pred = []
            
            if model_name == "Naive Bayes (CountVec)":
                vec_filename = "sst5_countvec_vectorizer.joblib"
                if not os.path.exists(vec_filename):
                    continue
                

                vectorizer = joblib.load(vec_filename)
                model = joblib.load(filename)
                
                # Transform v√† Predict
                X_test_vec = vectorizer.transform(X_test)
                y_pred = model.predict(X_test_vec)
                
            # --- X·ª¨ L√ù CHUNG CHO C√ÅC PIPELINE (SVM, XGB, v.v.) ---
            else:
                # C√°c model n√†y ƒë√£ ƒë√≥ng g√≥i c·∫£ vectorizer v√† model v√†o 1 pipeline
                pipeline = joblib.load(filename)
                y_pred = pipeline.predict(X_test)

            # --- T√çNH TO√ÅN METRICS ---
            acc = accuracy_score(y_test, y_pred)
            f1_macro = f1_score(y_test, y_pred, average='macro')
            f1_weighted = f1_score(y_test, y_pred, average='weighted')
            
            # L∆∞u report chi ti·∫øt
            report = classification_report(y_test, y_pred, target_names=label_map.values())
            report_path = os.path.join(OUTPUT_FOLDER, f"report_{model_name.replace(' ', '_').replace('(', '').replace(')', '')}.txt")
            with open(report_path, "w", encoding='utf-8') as f:
                f.write(f"Model: {model_name}\n")
                f.write(f"Test Accuracy: {acc:.4f}\n")
                f.write("-" * 30 + "\n")
                f.write(report)
            
            results.append({
                "Model": model_name,
                "Accuracy": acc,
                "Macro F1": f1_macro,
                "Weighted F1": f1_weighted
            })
            print(f"   ‚úÖ Done. Acc: {acc:.4f} | Macro F1: {f1_macro:.4f}")
            
        except Exception as e:
            print(f"   ‚ùå L·ªói ngo·∫°i l·ªá: {str(e)}")

    # --- 4. T·ªîNG H·ª¢P K·∫æT QU·∫¢ ---
    if results:
        df_results = pd.DataFrame(results)
        # S·∫Øp x·∫øp theo Macro F1 (ch·ªâ s·ªë quan tr·ªçng nh·∫•t cho Imbalanced Data)
        df_results = df_results.sort_values(by="Macro F1", ascending=False)
        
        print("\n" + "="*50)
        print("üèÜ B·∫¢NG X·∫æP H·∫†NG MODEL (Tr√™n t·∫≠p Test)")
        print("="*50)
        print(df_results.to_string(index=False))
        
        # L∆∞u CSV
        csv_path = os.path.join(OUTPUT_FOLDER, "final_comparison.csv")
        df_results.to_csv(csv_path, index=False)
        print(f"\nüíæ ƒê√£ l∆∞u b·∫£ng so s√°nh: {csv_path}")
        
        # V·∫º BI·ªÇU ƒê·ªí
        plt.figure(figsize=(12, 6))
        sns.set_style("whitegrid")
        
        # Chuy·ªÉn d·ªØ li·ªáu sang d·∫°ng d√†i ƒë·ªÉ v·∫Ω grouped bar chart
        df_melted = df_results.melt(id_vars="Model", value_vars=["Accuracy", "Macro F1"], var_name="Metric", value_name="Score")
        
        chart = sns.barplot(data=df_melted, x="Model", y="Score", hue="Metric", palette="viridis")
        plt.title("So s√°nh hi·ªáu su·∫•t c√°c m√¥ h√¨nh NLP (Non-Neural) tr√™n SST-5")
        plt.ylim(0, 0.65) # SST-5 baseline th∆∞·ªùng < 0.6, set limit ƒë·ªÉ d·ªÖ nh√¨n
        plt.xticks(rotation=15)
        plt.legend(loc='lower right')
        
        # Hi·ªÉn th·ªã s·ªë tr√™n c·ªôt
        for container in chart.containers:
            chart.bar_label(container, fmt='%.3f', padding=3, fontsize=9)
            
        plt.tight_layout()
        plot_path = os.path.join(OUTPUT_FOLDER, "model_benchmark_chart.png")
        plt.savefig(plot_path)
        print(f"üìä ƒê√£ l∆∞u bi·ªÉu ƒë·ªì: {plot_path}")
    else:
        print("\n‚ö†Ô∏è Kh√¥ng c√≥ k·∫øt qu·∫£ n√†o ƒë∆∞·ª£c ghi nh·∫≠n.")

if __name__ == "__main__":
    main()